# 数学讲解视频流水线 - 技术设计

## Context

- **背景**：用户输入数学题目，系统需产出带旁白与数学动画的讲解视频。方案依赖 LLM 做题目拆解与 Manim 代码生成、TTS 做旁白、FFmpeg 做合成；Manim 代码由 LLM 生成，存在语法/逻辑错误风险，需自愈机制。
- **当前状态**：从零构建，无既有代码约束；技术选型参考 aa.md（LLM、Manim、Edge-TTS/Azure/OpenAI TTS、FFmpeg，可选 SymPy、Stable Diffusion）。**LLM 调用统一使用 LangChain 框架**（题目分析、脚本生成、代码自愈均通过 LangChain 接入 ChatModel）。
- **约束**：Manim 渲染耗 CPU、可能崩溃；须先得到 TTS 时长再注入 Manim，以保证口型/节奏对齐；所有密钥与路径通过配置管理。

## Goals / Non-Goals

**Goals:**

- 实现四阶段流水线：题目分析 → 多模态脚本生成 → 素材生成（TTS + Manim 渲染 + 可选 SD）→ 视频合成。
- 对 Manim 代码实现「执行 → 捕获错误 → LLM 修复 → 重试」的自愈循环，可配置最大重试次数与超时。
- 时间对齐：TTS 先于 Manim 渲染，将各步语音时长注入 Manim 的 `self.wait()`（或等价机制）。
- 提供 HTTP API 触发生成并返回任务 ID 或结果路径，便于后续扩展轮询/回调。
- 提供 **Web 界面**：用户可上传或输入题目、触发生成、查看进度，生成完成后在页面内播放讲解视频并支持下载。

**Non-Goals:**

- 首版不承诺 Stable Diffusion 集成（可占位或开关控制）。
- 不实现用户系统、登录与计费、多租户。
- 不实现分布式渲染集群；首版为单机编排。

## Decisions

### 1. 流水线编排方式

- **决策**：首版在 FastAPI 内串行调用各阶段（或使用 BackgroundTasks 异步执行），不引入消息队列。
- **理由**：降低复杂度，便于调试与部署；Manim 渲染耗时可通过「异步任务 + 轮询/回调」缓解体验。
- **备选**：Celery/Redis 等队列可放在后续迭代，当需要水平扩展或任务持久化时再引入。

### 2. Manim 代码自愈策略

- **决策**：在独立子进程中执行 Manim（写临时 .py 文件后 `subprocess` 调用 manim CLI），捕获 stderr/stdout；若退出码非 0 或抛出异常，将错误信息与当前代码一并发给 LLM 请求修复，重试至多 N 次（如 3），超过则失败。
- **理由**：避免错误代码阻塞主进程；LLM 可根据 Traceback 定位问题。
- **备选**：使用 AST 解析仅替换 `self.wait()` 以减少误伤，首版可用简单字符串替换，后续再优化。

### 3. 时长注入实现方式

- **决策**：首版用「按出现顺序替换 `self.wait()` 为 `self.wait(duration)`」的字符串/正则方式，durations 数组与步骤顺序一致；若步骤数与 wait 个数不一致，需定义兜底策略（如不足用默认 2s，多余忽略）。
- **理由**：实现快、无需依赖 AST 库；与 aa.md 伪代码一致。
- **备选**：使用 `ast` 模块精确替换，提高鲁棒性，可在后续任务中实现。

### 4. 配置与密钥管理

- **决策**：所有 API Key、端点、Manim/FFmpeg 路径均从环境变量或单一配置文件（如 `.env` + `pydantic-settings`）读取，不写死在代码中。
- **理由**：安全与部署灵活性。

### 5. 合成阶段输入约定

- **决策**：合成接口接收「一段 Manim 视频路径 + 一段完整旁白音频路径」；若多步对应多段音频，由调用方或上游先拼接为单音频文件再传入。
- **理由**：与 aa.md 中「简单起见，一段长视频 + 一段长音频」一致；简化 FFmpeg 调用（`-shortest` 等）。

### 6. LLM 调用框架

- **决策**：所有大模型调用（题目分析、多模态脚本生成、Manim 代码自愈修复）统一通过 **LangChain** 完成。使用 LangChain 的 LCEL、ChatModel 与结构化输出（如 `with_structured_output` 或 Pydantic 绑定），便于切换后端（OpenAI/Claude 等）、复用 Prompt 与链式调用，并为后续扩展 Agent 或工具调用留空间。
- **理由**：统一抽象、易换模型、社区生态与文档完善；与 Python 3.10+ 及 pydantic-settings 兼容。
- **备选**：直接使用各厂商 SDK（如 openai、anthropic）——实现简单但多处重复、换模型需改多份代码。

### 7. Web 界面技术选型

- **决策**：前端采用**静态 HTML + CSS + JavaScript**，由 FastAPI 挂载静态目录（如 `StaticFiles`）提供；页面包含：题目输入/上传（文本框或文件上传）、提交生成按钮、生成状态/进度展示（轮询任务状态接口）、生成完成后内嵌视频播放器（`<video>` 或兼容的播放控件）及下载链接。生成任务采用**异步**（BackgroundTasks 或任务 ID），前端通过轮询 `GET /tasks/{task_id}` 或等价接口获取状态与结果视频 URL。
- **理由**：零构建、与 FastAPI 同源部署简单；满足「上传题目、生成后播放」的核心需求；后续可替换为 Vue/React 等而不影响 API 契约。
- **备选**：独立前端工程（Vue/React）+ 开发时代理或生产分离部署——首版不采纳以控制复杂度。

## Risks / Trade-offs

| 风险 | 缓解 |
|------|------|
| LLM 生成的 Manim 代码多次修复仍失败 | 限制重试次数并明确返回错误；后续可引入模板库，只让 LLM 填参数。 |
| Manim 渲染耗时导致请求超时 | 使用异步任务 + 任务 ID，客户端轮询或回调获取结果；必要时调大超时或仅支持「提交任务」不等待。 |
| TTS/LLM 外部服务不可用或限流 | 重试与退避；在 API 层返回明确错误码与提示。 |
| 时长注入与步骤数不一致 | 在 asset-generation 中定义明确规则（如按步数截断或补全 durations），并在 spec 中写清。 |
| 长时间生成导致浏览器等待超时 | 使用异步任务 + 轮询；前端展示「生成中」状态与可选取消，不依赖单次请求长时间挂起。 |

## Migration Plan

- 本项目为全新模块，无存量迁移。部署步骤：安装依赖（Python、Manim、FFmpeg、TTS 相关）、配置环境变量、启动 FastAPI；若使用 BackgroundTasks，单实例即可。
- 回滚：无数据迁移，回滚即停止服务或回退代码版本。

## Open Questions

- 多步时是否每步单独渲染为短视频再 FFmpeg concat（提高容错）？首版可先采用「一整段 Manim 场景 + 一整段音频」简化实现，该优化放入后续迭代。
- SD 集成优先级与接口形态（同步调用 vs 队列）待产品侧确认后可在 design 中补充。
